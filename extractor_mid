</div>
</div>
<div class="row">
<div class="col-md-12">
<p><strong>为什么要进行正文提取？</strong></p><p>对于一个单独的Web网页，往往最有价值的部分是网页的正文。然而就现在的大多数的网站的网页而言，不仅仅包含正文，网页标签等，其他的如广告，网页链接，插件等占据了网页相当一部分的内容。由于现实的需要，我们往往需要对网页的内容进行分析从而提取有价值的信息。一个网页的内容基本包含在正文中，对于新闻类网页尤其。将网页正文之外其他的内容剔除从而降低分析的难度是一种基本的思路。同时正文内容提取的好坏直接影响到接下来分析工作的质量。如何使用最快捷的方法来获取网页的正文同时又保证提取的内容的准确性？这就是我们这篇文章所讨论的主题。</p><p><strong>网页正文提取的思路及算法</strong></p><p>作为一个刚开始对正文提取没有什么认识的人，比如我，第一想法就是利用HTML网页的标签来判断正文的位置。因为根据网页的编辑和布局习惯，一般会将网页在正文部分放入到&lt;p&gt;,&lt;table&gt;以及&lt;div&gt; 中，并且应该包含&lt;title&gt;和&lt;head&gt;等标签。如果能够利用正则表达式首先找到这些标签的位置，并做相应的判断即可找到正文的内容位置。这是第一种思路，基于标签。</p><p><strong>1. 基于标签</strong></p><p>这是很容易想到的一种思路。基于HTML文件本身的某些结构特点和规范。这里也可以细分：</p><ul style="padding: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 25px; list-style-type: none; line-height: 28.796875px; font-family: &#39;Helvetica Neue&#39;, Helvetica, Tahoma, Arial, STXihei, &#39;Microsoft YaHei&#39;, 微软雅黑, sans-serif; white-space: normal;"><li>基于标签窗的算法： 例如&lt;p&gt;段落标签，一般的正文内容都会嵌套其中。&lt;title&gt;标题标签，一般会将论文的标题嵌入，接下来的内容很大程度上应该是正文的内容了。根据写作习惯，标题下方开始写正文，上方&lt;p&gt;标签就应该会频繁出现，主要的思路是标签的某种组合和标签的内容对应。</li><li>基于标签密度的算法：对于大多数的网页而言，正文部分的标签明显要少于非正文部分，而且正文当中的标签大部分标签应该属于修饰类标签。这就有一种基于标签统计的思路算法。从某一行开始，标签数量明显减少，到某一行又骤升，这一间隔区域很大程度上就属于网页的正文部分。</li><li>基于DOM树的算法：</li></ul><p>什么叫DOM(Document Object Model)树：并没有看到对DOM树的定义，不过很容易理解，下图就很明白体现了一种对应关系。</p><p>在一个网页中标签的位置下左图所示，而对应的DOM树如下又图（图片来源：http://www.w3school.com.cn/htmldom/dom_nodes.asp）所示：</p><p><img src="http://img0.tuicool.com/UF3ui2.jpg" alt="" style="max-width: 550px; height: auto; border-style: none; text-align: center; margin: 0px auto; display: block;"/><img src="http://img1.tuicool.com/rAZryu.jpg" alt="" style="max-width: 550px; height: auto; border-style: none; text-align: center; margin: 0px auto; display: block;"/><br/></p><p>&nbsp;&nbsp;这种算法需要对网站HTML建立DOM树，然后对之进行遍历递归，去除相应的噪音信息之后再从剩余的节点中进行选择。由于要建立DOM树，算法的时间/空间复杂度均较高。</p><p>基于标签的算法都潜在默认了这样的一个信息：即网站的网页生成，制作都遵循了一定的标签使用规范。不过现在的互联网网页五花八门，很难都按常理出牌，所以这在一定程度上降低了算法的准确性和通用性。&nbsp;<br/></p><p><strong>2.&nbsp;基于内容</strong></p><p>网页按照内容形式分类大概分为：主题型，图片型和目录型。</p><p>对于主体型的网页，例如新闻类，博客类等，主要特点是文字内容比较多。基于这一点，&nbsp;另外一种正文提取思路是基于正文本身的特点。在一定程度上，正文的文字数量要比其他部分多。这在一定程度上有助于形成了区域的区分度。文字数量的骤增和骤减在一定程度上可以作为正文开始和介绍的判读点。</p><p>这类算法在本质上没有多大的差异，只是选择度量文字密度的方式不同而已。有的是基于块，有的是基于行，有的是基于转化函数。算法都很容易理解，也相对比较容易实现。</p><p>下面的几篇文章就是基于网页内容的算法。</p><p>《基于行块分布函数的通用网页正文抽取》陈鑫</p><p>《基于网页分块的正文信息提取方法》&nbsp;黄 玲,陈 龙</p><p>基于内容正文提取的实现Java版:演示地址</p><p><a href="http://www.weixinxi.wang/open/extract.html" target="_blank">http://www.weixinxi.wang/open/extract.html</a></p><p><strong>3. 基于视觉</strong></p><p>想对于前面两种思路，这类算法的思路有一种&quot;高大上&quot;的感觉。这里不得不提到这类算法的基础：VIPS(Vision-based Page Segementation)算法。</p><p>VIPS算法：利用背景颜色，字体颜色和大小，边框，逻辑块和逻辑块之间的间距等视觉特征，制定相应的规则把页面分割成各个视觉块！(视觉效果真的是千变万化，如何制定规则集始终是个复杂的问题)</p><p>VIPS算法充分利用了Web页面的布局特征。它首先从DOM 树中提取出所有合适的页面块，然后根据这些页面块检测出它们之间所有的分割条，包括水平和垂直方向；最后基于这些分割条．重新构建Web页面的语义结构。对于每一个语义块又可以使用VIPS算法继续分割为更小的语义块。该算法分为页面块提取、分隔条提取和语义块重构3部分，并且是递归调用的过程，直到条件不满足为止.</p><p>相关文献：</p><p>《基于视觉特征的网页正文提取方法研究》安增文，徐杰锋</p><p>《A vision—based page segmentation algorithm》</p><p><strong>4. 基于数据挖掘/机器学习</strong></p><p>看到很多作者对这一思路的普遍评价是&quot;杀鸡焉用牛刀&quot;。</p><p>基本思路是使用一定数量的网页作为训练集，通过训练得到网页正文的一些特点，然后将这些特征作为网页片段是否符合网页正文的判断依据。对于数据挖掘/机器学习算法来讲，训练样本的采集很重要，然而现实是互联网中网页形式千变万化，不太可能取太多数量作为训练样本。这样这种算法的准确性和通用性就受到了制约，同时这类算法前期工作也比较复杂。</p><p><br/></p>
